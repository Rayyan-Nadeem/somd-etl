{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6b2ec7",
   "metadata": {},
   "source": [
    "# ETL Process for CSV Files\n",
    "This notebook demonstrates an ETL process for the CSV files in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "082f2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bf806d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EMAIL = 'na@test.com'\n",
    "DEFAULT_PHONE = '000-000-0000'\n",
    "DEFAULT_DOB = '01/01/1900'\n",
    "DEFAULT_STREET = '123 Main St'\n",
    "DEFAULT_CITY = 'Anytown'\n",
    "DEFAULT_STATE = 'NY'\n",
    "DEFAULT_ZIP = '20000'\n",
    "DEFAULT_SEX = 'Other'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba46228",
   "metadata": {},
   "source": [
    "## Load CSV Files\n",
    "Load the CSV files from the directory into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbc1dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AthleteId', 'Parent2Email', 'GrpHome', 'Active', 'LastName', 'FirstName', 'Sex', 'DateOfBirth', 'SSN', 'ParentName', 'ParentAddress', 'ParentCity', 'ParentState', 'ParentZIP', 'ParentPhone', 'ParentCell', 'ParentEMail', 'Parent2Name', 'Parent2Address', 'Parent2City', 'Parent2State', 'Parent2Zip', 'Parent2Phone', 'Parent2Cell', 'HomePhone', 'EmergencyNbr', 'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'AthleteEMail', 'CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName', 'DrNamePhone', 'Downs', 'AaAx', 'NoXray', 'Photo', 'Na/Rl', 'Allgeries', 'Tetnus', 'Medication', 'Comments', 'MedicalDeadline', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'AwardsComments', 'Deactivation_Dt']\n",
      "   AthleteId            Parent2Email GrpHome  Active              LastName  \\\n",
      "0       1445     carterg22@gmail.com      NO    True                   NaN   \n",
      "1       1320                     NaN      NO   False                   NaN   \n",
      "2       1477                     NaN      NO    True                   NaN   \n",
      "3       1034                     NaN      NO    True            Abdelhalim   \n",
      "4         18  khalil@shuraforall.org      No    True  Abdul-Rahman (Flynn)   \n",
      "\n",
      "  FirstName  Sex DateOfBirth  SSN       ParentName  ...     Tetnus  \\\n",
      "0       NaN    F   30-Jan-14  NaN      Tori Carter  ...        NaN   \n",
      "1       NaN  NaN         NaN  NaN              NaN  ...        NaN   \n",
      "2       NaN    M   13-Aug-12  NaN  Masooma Hasnain  ...        NaN   \n",
      "3      Mona    F   25-Sep-01  NaN   Fatima H Kahil  ...        NaN   \n",
      "4   Ahleyah    F   20-Nov-83  NaN      Joyce Flynn  ...  15-Aug-88   \n",
      "\n",
      "                                          Medication  \\\n",
      "0  Zoloft, 75mg, qd\\nClanidine, 2mg, qhs\\nExlax, ...   \n",
      "1                                                NaN   \n",
      "2                                                NaN   \n",
      "3  insulin aspert, 3xday\\ndexem G7, every 10days\\...   \n",
      "4  clonazapam, 0.5mg, 2xday\\nLevothyroxine, 50mcg...   \n",
      "\n",
      "                                            Comments MedicalDeadline  \\\n",
      "0                                             Autism       05-Mar-27   \n",
      "1                                                NaN             NaN   \n",
      "2                                             Autism       26-Jun-27   \n",
      "3                                                NaN       14-Aug-27   \n",
      "4  Glasses, Repaired CHD as infant, reactive airw...       26-Oct-26   \n",
      "\n",
      "  PhotoRelease AlternatePhone  EntryDate TorchRunner AwardsComments  \\\n",
      "0        False            NaN  3/18/2024       False            NaN   \n",
      "1        False            NaN  2/17/2020       False            NaN   \n",
      "2        False            NaN  8/10/2024       False            NaN   \n",
      "3        False            NaN  3/10/2017       False            NaN   \n",
      "4        False            NaN  4/25/1995       False            NaN   \n",
      "\n",
      "  Deactivation_Dt  \n",
      "0             NaN  \n",
      "1             NaN  \n",
      "2             NaN  \n",
      "3             NaN  \n",
      "4             NaN  \n",
      "\n",
      "[5 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load CSV files\n",
    "athletes_df = pd.read_csv('athletes.csv', encoding='latin1')\n",
    "\n",
    "# Display the first rows of the DataFrame\n",
    "print(athletes_df.columns.tolist())\n",
    "print(athletes_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf89f10",
   "metadata": {},
   "source": [
    "## Normalization and Cleaner functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84219a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5ee24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize phone numbers\n",
    "def normalize_phone(phone):\n",
    "    if pd.notna(phone):\n",
    "        phone = str(phone).strip()\n",
    "        # Remove non-numeric characters\n",
    "        phone = re.sub(r'\\D', '', phone)\n",
    "        # Format to standard 10-digit format\n",
    "        if len(phone) == 10:\n",
    "            return f\"{phone[:3]}-{phone[3:6]}-{phone[6:]}\"\n",
    "    return DEFAULT_PHONE\n",
    "\n",
    "# Define a function to extract phone numbers from a string\n",
    "def extract_phone(s):\n",
    "    if pd.isna(s):\n",
    "        return None, None\n",
    "    s = str(s)\n",
    "    # Regex to find phone numbers in various formats\n",
    "    phone_match = re.search(r'(\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b)', s)\n",
    "    if phone_match:\n",
    "        phone = phone_match.group(1)\n",
    "        # Remove the phone number from the string to get the name\n",
    "        name = s.replace(phone, '').strip(', /-')\n",
    "        return name, phone\n",
    "    return s, None\n",
    "\n",
    "# Define a function to normalize email\n",
    "def normalize_email(email):\n",
    "    if pd.notna(email) and '@' in str(email):\n",
    "        return str(email).strip().lower()\n",
    "    return DEFAULT_EMAIL\n",
    "\n",
    "# Define a function to split full names into first and last names\n",
    "def split_name(full_name):\n",
    "    if pd.notna(full_name) and full_name.strip():\n",
    "        parts = full_name.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            return parts[0], ' '.join(parts[1:])\n",
    "        return parts[0], ''\n",
    "    return 'N/A', 'N/A'\n",
    "\n",
    "def counselor_exists(row):\n",
    "    first_name = row.get('CounselorfirstName')\n",
    "    last_name = row.get('CounselorlastName')\n",
    "    return pd.notna(first_name) and first_name not in ['', 'N/A'] and pd.notna(last_name) and last_name not in ['', 'N/A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635eb727",
   "metadata": {},
   "source": [
    "# Export Parents/Guardians Data\n",
    "Create a separate CSV file for parents/guardians, linking them to athletes using AthleteId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c67e11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping parent record id=224 due to missing name: firstName='465-6212', lastName=''\n",
      "Dropping parent record id=601 due to missing name: firstName='ARC', lastName=''\n",
      "Dropping parent record id=124 due to missing name: firstName='Donna', lastName=''\n",
      "Mapped parent data saved to athlete_parents.csv\n"
     ]
    }
   ],
   "source": [
    "# Define a function to map parent/guardian data based on a prefix\n",
    "def map_parent(row, prefix):\n",
    "    if prefix == \"Parent2\":\n",
    "        email_key = \"Parent2Email\"\n",
    "        zip_col = f\"{prefix}Zip\"\n",
    "    else:\n",
    "        email_key = f\"{prefix}EMail\"\n",
    "        zip_col = f\"{prefix}ZIP\"\n",
    "\n",
    "    first_name, last_name = split_name(row[f\"{prefix}Name\"])\n",
    "\n",
    "    return {\n",
    "        \"id\": row[\"AthleteId\"],\n",
    "        \"firstName\": first_name,\n",
    "        \"lastName\": last_name,\n",
    "        \"primaryPhone\": normalize_phone(row[f\"{prefix}Phone\"]),\n",
    "        \"secondaryPhone\": (lambda p: p if p != DEFAULT_PHONE else \"\")(\n",
    "            normalize_phone(row.get(f\"{prefix}Cell\"))\n",
    "        ),\n",
    "        \"email\": normalize_email(row.get(email_key)),\n",
    "        \"street\": row.get(f\"{prefix}Address\"),\n",
    "        \"city\": row.get(f\"{prefix}City\"),\n",
    "        \"state\": row.get(f\"{prefix}State\"),\n",
    "        \"zip\": row.get(zip_col)\n",
    "    }\n",
    "\n",
    "\n",
    "mapped_parents = []\n",
    "# Iterate over each row in the athletes_df DataFrame\n",
    "for _, row in athletes_df.iterrows():\n",
    "    # Map Parent 1 if required fields are present\n",
    "    if pd.notna(row[\"ParentName\"]) and pd.notna(row[\"ParentPhone\"]):\n",
    "        mapped_parents.append(map_parent(row, \"Parent\"))\n",
    "    # Map Parent 2 if required fields are present\n",
    "    if pd.notna(row[\"Parent2Name\"]) and pd.notna(row[\"Parent2Phone\"]):\n",
    "        mapped_parents.append(map_parent(row, \"Parent2\"))\n",
    "\n",
    "# Convert the mapped data to a DataFrame and export to CSV\n",
    "mapped_parents_df = pd.DataFrame(mapped_parents)\n",
    "\n",
    "# Filter out rows missing first or last name\n",
    "mask_missing = (\n",
    "    mapped_parents_df['firstName'].isna() |\n",
    "    mapped_parents_df['lastName'].isna() |\n",
    "    (mapped_parents_df['firstName'] == '') |\n",
    "    (mapped_parents_df['lastName'] == '') |\n",
    "    (mapped_parents_df['firstName'] == 'N/A') |\n",
    "    (mapped_parents_df['lastName'] == 'N/A')\n",
    ")\n",
    "if mask_missing.any():\n",
    "    for _, row in mapped_parents_df[mask_missing].iterrows():\n",
    "        print(\n",
    "            f\"Dropping parent record id={row['id']} \"\n",
    "            f\"due to missing name: firstName={row['firstName']!r}, \"\n",
    "            f\"lastName={row['lastName']!r}\"\n",
    "        )\n",
    "    mapped_parents_df = mapped_parents_df.loc[~mask_missing].copy()\n",
    "\n",
    "# Save to CSV and print confirmation\n",
    "mapped_parents_df.to_csv('athlete_parents.csv', index=False)\n",
    "print('Mapped parent data saved to athlete_parents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f912bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent-related columns removed.\n"
     ]
    }
   ],
   "source": [
    "# Strip parent-related columns from athletes_df now that they are no longer needed\n",
    "columns_to_drop = ['ParentName', 'ParentPhone', 'ParentCell', 'ParentEMail', 'ParentAddress', 'ParentCity', 'ParentState', 'ParentZIP',\n",
    "                  'Parent2Name', 'Parent2Phone', 'Parent2Cell', 'Parent2Email', 'Parent2Address', 'Parent2City', 'Parent2State', 'Parent2Zip']\n",
    "\n",
    "cols_lower = {col.lower(): col for col in athletes_df.columns}\n",
    "existing_columns = [cols_lower[c.lower()] for c in ['ParentName', 'ParentPhone', 'ParentCell', 'ParentEMail', 'ParentAddress', 'ParentCity', 'ParentState', 'ParentZIP',\n",
    "                                                     'Parent2Name', 'Parent2Phone', 'Parent2Cell', 'Parent2Email', 'Parent2Address', 'Parent2City', 'Parent2State', 'Parent2Zip']\n",
    "                    if c.lower() in cols_lower]\n",
    "if existing_columns:\n",
    "    athletes_df.drop(existing_columns, axis=1, inplace=True)\n",
    "    print(\"Parent-related columns removed.\")\n",
    "else:\n",
    "    print(\"No parent-related columns to remove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4559535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AthleteId', 'GrpHome', 'Active', 'LastName', 'FirstName', 'Sex', 'DateOfBirth', 'SSN', 'HomePhone', 'EmergencyNbr', 'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'AthleteEMail', 'CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName', 'DrNamePhone', 'Downs', 'AaAx', 'NoXray', 'Photo', 'Na/Rl', 'Allgeries', 'Tetnus', 'Medication', 'Comments', 'MedicalDeadline', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'AwardsComments', 'Deactivation_Dt']\n"
     ]
    }
   ],
   "source": [
    "# print just the headers of the DataFrame\n",
    "print(athletes_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500744d",
   "metadata": {},
   "source": [
    "## Export Counselor Data\n",
    "Create a separate CSV file for counselors, linking them to athletes using AthleteId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0348412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an improved function to split agency name and phon\n",
    "def split_agency_name(agency):\n",
    "    if pd.notna(agency):\n",
    "        # Try different regex patterns to extract phone numbers\n",
    "        # Pattern 1: Name, phone format\n",
    "        match = re.search(r'(.*?)[, /\\s]+(\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4})$', agency.strip())\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        \n",
    "        # Pattern 2: Name/phone format\n",
    "        match = re.search(r'(.*?)[/](\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4})$', agency.strip())\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "            \n",
    "        # Pattern 3: Name (phone) format\n",
    "        match = re.search(r'(.*?)\\s*\\(\\s*(\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4})\\s*\\)', agency.strip())\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "        \n",
    "        # No phone number found, return just the name\n",
    "        return agency.strip(), None\n",
    "    return None, None\n",
    "\n",
    "# Define a function to normalize agency names (remove trailing commas, etc.)\n",
    "def normalize_agency_name(name):\n",
    "    if pd.notna(name):\n",
    "        # Remove trailing commas, slashes, etc.\n",
    "        return re.sub(r'[,/]$', '', name.strip())\n",
    "    return None\n",
    "\n",
    "# Function to map common agency names to a standard form\n",
    "def standardize_agency_name(name):\n",
    "    if pd.notna(name):\n",
    "        # Map of common variations to standard names\n",
    "        agency_map = {\n",
    "            'Linwood': 'Linwood Center',\n",
    "            'Linwood,': 'Linwood Center',\n",
    "            'ARC': 'Arc of Howard County',\n",
    "            'ARC/': 'Arc of Howard County',\n",
    "            'ARC /': 'Arc of Howard County',\n",
    "            'Athelas': 'Athelas Institute',\n",
    "            'Athelas,': 'Athelas Institute',\n",
    "            'Athlelas,': 'Athelas Institute',\n",
    "            'Athelas Institute,': 'Athelas Institute'\n",
    "        }\n",
    "        \n",
    "        # Check if the name is a known variation\n",
    "        clean_name = normalize_agency_name(name)\n",
    "        if clean_name in agency_map:\n",
    "            return agency_map[clean_name]\n",
    "        return clean_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c830eec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in DataFrame: ['AthleteId', 'GrpHome', 'Active', 'LastName', 'FirstName', 'Sex', 'DateOfBirth', 'SSN', 'HomePhone', 'EmergencyNbr', 'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'AthleteEMail', 'CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName', 'DrNamePhone', 'Downs', 'AaAx', 'NoXray', 'Photo', 'Na/Rl', 'Allgeries', 'Tetnus', 'Medication', 'Comments', 'MedicalDeadline', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'AwardsComments', 'Deactivation_Dt']\n"
     ]
    }
   ],
   "source": [
    "#Check for missing columns\n",
    "print(\"Available columns in DataFrame:\", athletes_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db55056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counselor data saved to athlete_counselors.csv\n",
      "Dropped counselor columns: ['CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName', 'GrpHome']\n"
     ]
    }
   ],
   "source": [
    "# Create a copy for counselor processing\n",
    "counselors_df = athletes_df[['AthleteId', 'CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName']].copy().dropna(subset=['CounselorName'])\n",
    "counselors_df.rename(columns={'AthleteId': 'id'}, inplace=True)\n",
    "\n",
    "# Apply extraction and cleaning functions for counselor name and phone\n",
    "counselors_df[['CounselorName_clean', 'phone_from_name']] = counselors_df['CounselorName'].apply(lambda x: pd.Series(extract_phone(x)))\n",
    "counselors_df['phone'] = counselors_df['CounselorPhone'].fillna(counselors_df['phone_from_name']).apply(normalize_phone)\n",
    "counselors_df['email'] = counselors_df['CounselorEMail'].apply(normalize_email)\n",
    "counselors_df[['firstName', 'lastName']] = counselors_df['CounselorName_clean'].apply(lambda x: pd.Series(split_name(x)))\n",
    "\n",
    "# Apply extraction and cleaning functions for agency name and phone\n",
    "counselors_df[['agencyName', 'agencyPhone']] = counselors_df['AgencyName'].apply(lambda x: pd.Series(split_agency_name(x)))\n",
    "counselors_df['agencyName'] = counselors_df['agencyName'].apply(standardize_agency_name)\n",
    "\n",
    "# Create a map of the first phone number for each agency\n",
    "agency_phone_map = counselors_df.dropna(subset=['agencyName', 'agencyPhone']).groupby('agencyName')['agencyPhone'].first().to_dict()\n",
    "\n",
    "# Fill missing agency phones using the map\n",
    "counselors_df['agencyPhone'] = counselors_df['agencyName'].map(agency_phone_map).fillna(counselors_df['agencyPhone'])\n",
    "\n",
    "# Normalize the agency phone numbers\n",
    "counselors_df['agencyPhone'] = counselors_df['agencyPhone'].apply(normalize_phone)\n",
    "\n",
    "# If an agency name exists, a phone must be present; otherwise, slash them\n",
    "mask = counselors_df['agencyName'].notna() & (counselors_df['agencyPhone'] == DEFAULT_PHONE)\n",
    "counselors_df.loc[mask, ['agencyName', 'agencyPhone']] = [None, None]\n",
    "\n",
    "# Filter out rows that are likely not names\n",
    "counselors_df = counselors_df[~counselors_df['firstName'].str.contains('Adaptive|Living|Center|School|Rehab|Apartment|Arc', na=False, case=False)]\n",
    "\n",
    "# Prefix the split name and contact columns with “Counselor”\n",
    "counselors_df.rename(columns={\n",
    "    'firstName': 'CounselorfirstName',\n",
    "    'lastName':  'CounselorlastName',\n",
    "    'email':     'Counseloremail',\n",
    "    'phone':     'Counselorphone'\n",
    "}, inplace=True)\n",
    "# Select and rename final columns\n",
    "final_counselors_df = counselors_df[['id', 'CounselorfirstName', 'CounselorlastName', 'Counseloremail', 'Counselorphone', 'agencyName', 'agencyPhone']].copy()\n",
    "\n",
    "# Drop rows where firstName and lastName are both missing/invalid\n",
    "final_counselors_df.dropna(subset=['CounselorfirstName', 'CounselorlastName'], how='all', inplace=True)\n",
    "final_counselors_df = final_counselors_df[final_counselors_df['CounselorfirstName'] != '']\n",
    "\n",
    "# Save the processed counselors data\n",
    "final_counselors_df.to_csv('athlete_counselors.csv', index=False)\n",
    "print('Counselor data saved to athlete_counselors.csv')\n",
    "\n",
    "# Drop counselor-related columns from athletes_df\n",
    "counselor_columns = ['CounselorName', 'CounselorEMail', 'CounselorPhone', 'AgencyName', 'GrpHome']\n",
    "existing_counselor_columns = [col for col in counselor_columns if col in athletes_df.columns]\n",
    "if existing_counselor_columns:\n",
    "    athletes_df.drop(existing_counselor_columns, axis=1, inplace=True)\n",
    "    print(f\"Dropped counselor columns: {existing_counselor_columns}\")\n",
    "else:\n",
    "    print(\"No counselor columns to drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b526d9",
   "metadata": {},
   "source": [
    "## Export Athlete Data\n",
    "Create a separate CSV file for athletes matching the simplified schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902db038",
   "metadata": {},
   "source": [
    "## Normalize DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09d8a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'DateOfBirth' column cleaned and formatted.\n",
      "  DateOfBirth\n",
      "0  01/30/2014\n",
      "1  01/01/1900\n",
      "2  08/13/2012\n",
      "3  09/25/2001\n",
      "4  11/20/1983\n"
     ]
    }
   ],
   "source": [
    "# Inspect and clean 'DateOfBirth' column\n",
    "if 'DateOfBirth' in athletes_df.columns:\n",
    "    # Replace invalid or missing string values with pd.NA\n",
    "    athletes_df['DateOfBirth'] = athletes_df['DateOfBirth'].replace(['', 'None', 'N/A', 'NaN'], pd.NA)\n",
    "\n",
    "    # Convert to datetime, coercing errors to NaT (Not a Time)\n",
    "    athletes_df['DateOfBirth'] = pd.to_datetime(athletes_df['DateOfBirth'], errors='coerce', format='%d-%b-%y')\n",
    "\n",
    "    # Define a function to adjust years that are incorrectly parsed into the future\n",
    "    def fix_year(date):\n",
    "        if pd.notna(date) and date.year > 2025:  # Assuming no valid DOB is in the future\n",
    "            return date.replace(year=date.year - 100)\n",
    "        return date\n",
    "\n",
    "    # Apply the year correction\n",
    "    athletes_df['DateOfBirth'] = athletes_df['DateOfBirth'].apply(fix_year)\n",
    "\n",
    "    # Fill any remaining NaT values with the default DOB\n",
    "    athletes_df['DateOfBirth'] = athletes_df['DateOfBirth'].fillna(pd.to_datetime(DEFAULT_DOB, format='%m/%d/%Y'))\n",
    "\n",
    "    # Format the final date strings to MM/DD/YYYY\n",
    "    athletes_df['DateOfBirth'] = athletes_df['DateOfBirth'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    print(\"'DateOfBirth' column cleaned and formatted.\")\n",
    "    print(athletes_df[['DateOfBirth']].head())\n",
    "else:\n",
    "    print(\"Error: 'DateOfBirth' column is missing in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e763ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athlete data saved to athletes_simplified.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the required columns exist before extracting athlete data\n",
    "required_columns = ['AthleteId', 'FirstName', 'LastName', 'Sex', 'DateOfBirth', 'AthleteEMail', 'HomePhone', 'EmergencyNbr',\n",
    "                'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'Active', 'MedicalDeadline']\n",
    "\n",
    "\n",
    "# Extract athlete data matching the simplified schema\n",
    "athletes_schema_df = athletes_df[required_columns].copy()\n",
    "\n",
    "# Add uniform_size column with default value 's'\n",
    "athletes_schema_df['uniform_size'] = 's'\n",
    "\n",
    "# Define a function to normalize phone numbers (extract only digits)\n",
    "def normalize_phone_digits(phone):\n",
    "    if pd.isna(phone) or phone == '':\n",
    "        return ''\n",
    "    # Extract only digits from the phone number\n",
    "    return ''.join(filter(str.isdigit, str(phone)))\n",
    "\n",
    "# Normalize phone numbers to extract digits first\n",
    "athletes_schema_df['HomePhone'] = athletes_schema_df['HomePhone'].apply(normalize_phone_digits)\n",
    "athletes_schema_df['EmergencyNbr'] = athletes_schema_df['EmergencyNbr'].apply(normalize_phone_digits)\n",
    "\n",
    "# If HomePhone is missing, use EmergencyNbr\n",
    "athletes_schema_df['HomePhone'] = athletes_schema_df['HomePhone'].replace('', np.nan).fillna(athletes_schema_df['EmergencyNbr'])\n",
    "\n",
    "# Now, apply the final normalization that defaults to DEFAULT_PHONE\n",
    "athletes_schema_df['HomePhone'] = athletes_schema_df['HomePhone'].apply(normalize_phone)\n",
    "\n",
    "# Validate and normalize emails\n",
    "if 'AthleteEMail' in athletes_schema_df.columns:\n",
    "    athletes_schema_df['AthleteEMail'] = athletes_schema_df['AthleteEMail'].apply(normalize_email)\n",
    "\n",
    "# Fill missing address parts with default values\n",
    "for col, default in [('AthleteAddress', DEFAULT_STREET),\n",
    "                     ('AthleteCity', DEFAULT_CITY),\n",
    "                     ('AthleteState', DEFAULT_STATE),\n",
    "                     ('AthleteZip', DEFAULT_ZIP)]:\n",
    "    athletes_schema_df[col] = athletes_schema_df[col].replace('', np.nan).fillna(default)\n",
    "\n",
    "# Add county for Maryland athletes, defaulting to \"howard county\"\n",
    "athletes_schema_df['county'] = np.where(athletes_schema_df['AthleteState'] == 'MD', 'howard county', '')\n",
    "\n",
    "# default sex to Default value if not present\n",
    "if 'Sex' in athletes_schema_df.columns:\n",
    "    athletes_schema_df['Sex'] = athletes_schema_df['Sex'].replace('', np.nan).fillna(DEFAULT_SEX)\n",
    "\n",
    "athletes_schema_df.drop(columns=['EmergencyNbr'], inplace=True)\n",
    "\n",
    "# Rename columns to match the schema\n",
    "athletes_schema_df.rename(columns={\n",
    "    'AthleteId': 'id',\n",
    "    'FirstName': 'firstName',\n",
    "    'LastName': 'lastName',\n",
    "    'Sex': 'gender',\n",
    "    'DateOfBirth': 'dateOfBirth',\n",
    "    'AthleteEMail': 'email',\n",
    "    'HomePhone': 'phone', \n",
    "    'AthleteAddress': 'street',\n",
    "    'AthleteCity': 'city',\n",
    "    'AthleteState': 'state',\n",
    "    'AthleteZip': 'zip',\n",
    "    'Active': 'status',\n",
    "    'MedicalDeadline': 'medicalStatus'\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'status' to a more descriptive format\n",
    "athletes_schema_df['status'] = athletes_schema_df['status'].apply(lambda x: 'Active' if x else 'Inactive')\n",
    "\n",
    "# Format medical status dates from \"28-Mar-21\" to \"03/04/2023\" format\n",
    "def format_medical_date(date_str):\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return ''\n",
    "    try:\n",
    "        # Parse the date string\n",
    "        date_obj = pd.to_datetime(date_str, format='%d-%b-%y', errors='coerce')\n",
    "        if pd.isna(date_obj):\n",
    "            return date_str\n",
    "        \n",
    "        # Format as MM/DD/YYYY\n",
    "        return date_obj.strftime('%m/%d/%Y')\n",
    "    except:\n",
    "        return date_str\n",
    "\n",
    "athletes_schema_df['medicalStatus'] = athletes_schema_df['medicalStatus'].apply(format_medical_date)\n",
    "\n",
    "# Save to a new CSV file\n",
    "athletes_schema_df.to_csv('athletes_simplified.csv', index=False)\n",
    "print('Athlete data saved to athletes_simplified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425af002",
   "metadata": {},
   "source": [
    "## Drop Simplified Athlete Columns\n",
    "Remove columns that were exported to `athletes_simplified.csv`, excluding `FirstName`, `LastName` and `AthleteId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1b3bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['Sex', 'DateOfBirth', 'AthleteEMail', 'HomePhone', 'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'Active', 'MedicalDeadline', 'EmergencyNbr']\n",
      "Remaining headers: ['AthleteId', 'LastName', 'FirstName', 'SSN', 'DrNamePhone', 'Downs', 'AaAx', 'NoXray', 'Photo', 'Na/Rl', 'Allgeries', 'Tetnus', 'Medication', 'Comments', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'AwardsComments', 'Deactivation_Dt']\n"
     ]
    }
   ],
   "source": [
    "# Define columns to drop (excluding 'FirstName', 'LastName' and 'AthleteId')\n",
    "columns_to_drop = ['Sex', 'DateOfBirth', 'AthleteEMail', 'HomePhone',\n",
    "                   'AthleteAddress', 'AthleteCity', 'AthleteState', 'AthleteZip', 'Active', 'MedicalDeadline', 'EmergencyNbr']\n",
    "\n",
    "\n",
    "# Drop columns if they exist in the DataFrame\n",
    "existing_columns = [col for col in columns_to_drop if col in athletes_df.columns]\n",
    "if existing_columns:\n",
    "    athletes_df.drop(existing_columns, axis=1, inplace=True)\n",
    "    print(f\"Dropped columns: {existing_columns}\")\n",
    "else:\n",
    "    print(\"No matching columns to drop.\")\n",
    "\n",
    "# Print the remaining headers of the DataFrame\n",
    "print('Remaining headers:', athletes_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e52fac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athlete comments CSV exported as athlete_comments.csv\n",
      "Dropped comments columns from athletes_df\n",
      "Remaining headers: ['AthleteId', 'LastName', 'FirstName', 'SSN', 'DrNamePhone', 'Downs', 'AaAx', 'NoXray', 'Photo', 'Na/Rl', 'Allgeries', 'Tetnus', 'Medication', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'Deactivation_Dt']\n"
     ]
    }
   ],
   "source": [
    "# Check for comments columns and export them if they exist\n",
    "if 'Comments' in athletes_df.columns and 'AwardsComments' in athletes_df.columns:\n",
    "    comments_df = athletes_df[['AthleteId', 'Comments', 'AwardsComments']].copy()\n",
    "    comments_df.rename(columns={\n",
    "        'Comments': 'normal_comments',\n",
    "        'AwardsComments': 'award_comments'\n",
    "    }, inplace=True)\n",
    "    comments_df.to_csv('athlete_comments.csv', index=False)\n",
    "    print('Athlete comments CSV exported as athlete_comments.csv')\n",
    "    athletes_df.drop(['Comments', 'AwardsComments'], axis=1, inplace=True)\n",
    "    print('Dropped comments columns from athletes_df')\n",
    "else:\n",
    "    print('No comment columns found to export or drop')\n",
    "\n",
    "print('Remaining headers:', athletes_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc160b",
   "metadata": {},
   "source": [
    "## Extract Medical Data\n",
    "Create a separate DataFrame for medical information including doctor details and allergies/medication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32532acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical data saved to athlete_medical.csv\n",
      "\n",
      "Sample of medical data:\n",
      "     id         doctor_name  doctor_phone          Allgeries  \\\n",
      "0  1445  Dr. Michelle Mcwan  443-451-1600                NaN   \n",
      "1  1320                None                              NaN   \n",
      "2  1477       Dr. L. Berger  410-465-7550                NaN   \n",
      "3  1034         Dr. Hashimi  410-997-2770                NaN   \n",
      "4    18       Dr. Alice Lee                augmentin, gluten   \n",
      "\n",
      "                                          Medication  \n",
      "0  Zoloft, 75mg, qd\\nClanidine, 2mg, qhs\\nExlax, ...  \n",
      "1                                                NaN  \n",
      "2                                                NaN  \n",
      "3  insulin aspert, 3xday\\ndexem G7, every 10days\\...  \n",
      "4  clonazapam, 0.5mg, 2xday\\nLevothyroxine, 50mcg...  \n",
      "\n",
      "Dropped medical columns: ['DrNamePhone', 'Allgeries', 'Medication', 'Downs', 'AaAx', 'NoXray', 'Tetnus']\n",
      "\n",
      "Remaining headers: ['AthleteId', 'LastName', 'FirstName', 'SSN', 'Photo', 'Na/Rl', 'PhotoRelease', 'AlternatePhone', 'EntryDate', 'TorchRunner', 'Deactivation_Dt']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to split doctor name and phone number\n",
    "def split_doctor_info(doctor_info):\n",
    "    if pd.isna(doctor_info) or doctor_info == '':\n",
    "        return None, None\n",
    "    \n",
    "    # Common pattern: \"Dr. Name, Phone Number\"\n",
    "    match = re.search(r'(.*?)(?:,\\s*(\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4}))?$', doctor_info.strip())\n",
    "    if match:\n",
    "        name = match.group(1).strip() if match.group(1) else None\n",
    "        phone = match.group(2).strip() if match.group(2) else None\n",
    "        return name, phone\n",
    "    else:\n",
    "        return doctor_info, None\n",
    "\n",
    "# Create medical_df from athletes_df with required columns\n",
    "medical_columns = ['AthleteId', 'DrNamePhone', 'Allgeries', 'Medication']\n",
    "medical_df = athletes_df[medical_columns].copy()\n",
    "\n",
    "# Rename AthleteId to id\n",
    "medical_df.rename(columns={'AthleteId': 'id'}, inplace=True)\n",
    "\n",
    "# Split DrNamePhone into doctor_name and doctor_phone\n",
    "medical_df[['doctor_name', 'doctor_phone']] = medical_df['DrNamePhone'].apply(lambda x: pd.Series(split_doctor_info(x)))\n",
    "\n",
    "# If doctor_name is missing, slash both name and phone\n",
    "mask = medical_df['doctor_name'].isna() | (medical_df['doctor_name'].str.strip() == '')\n",
    "medical_df.loc[mask, ['doctor_name', 'doctor_phone']] = [None, None]\n",
    "\n",
    "# Normalize the doctor_phone column and ensure it's a string\n",
    "medical_df['doctor_phone'] = medical_df['doctor_phone'].apply(lambda x: str(normalize_phone(x)) if x else '')\n",
    "\n",
    "# Drop the original DrNamePhone column\n",
    "medical_df.drop(['DrNamePhone'], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to match the desired format\n",
    "medical_df = medical_df[['id', 'doctor_name', 'doctor_phone', 'Allgeries', 'Medication']]\n",
    "\n",
    "# Save the medical data to a CSV file\n",
    "medical_df.to_csv('athlete_medical.csv', index=False)\n",
    "print('Medical data saved to athlete_medical.csv')\n",
    "\n",
    "# Display a sample of the medical data\n",
    "print(\"\\nSample of medical data:\")\n",
    "print(medical_df.head())\n",
    "\n",
    "# Drop the medical columns from the athletes_df since they are now in their own file\n",
    "medical_columns_to_drop = ['DrNamePhone', 'Allgeries', 'Medication', 'Downs', 'AaAx', 'NoXray', 'Tetnus']\n",
    "existing_medical_columns = [col for col in medical_columns_to_drop if col in athletes_df.columns]\n",
    "if existing_medical_columns:\n",
    "    athletes_df.drop(existing_medical_columns, axis=1, inplace=True)\n",
    "    print(f\"\\nDropped medical columns: {existing_medical_columns}\")\n",
    "else:\n",
    "    print(\"\\nNo medical columns to drop\")\n",
    "\n",
    "# Print remaining columns\n",
    "print(\"\\nRemaining headers:\", athletes_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3237fd2",
   "metadata": {},
   "source": [
    "# Generate Nested JSON from CSVs\n",
    "This section generates a nested JSON file from the exported CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c771db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1150 athletes from simplified CSV\n",
      "Loaded 1150 medical records\n",
      "Loaded 104 counselor records\n",
      "Loaded 104 counselor records\n",
      "Loaded 1150 comment records\n",
      "Loaded 769 parent records\n",
      "Loaded 1150 comment records\n",
      "Loaded 769 parent records\n",
      "Created nested JSON file with 1150 athletes\n",
      "\n",
      "Sample of first athlete in nested JSON:\n",
      "{\n",
      "    \"id\": 1445,\n",
      "    \"firstName\": NaN,\n",
      "    \"lastName\": NaN,\n",
      "    \"gender\": \"F\",\n",
      "    \"dateOfBirth\": \"01/30/2014\",\n",
      "    \"email\": \"na@test.com\",\n",
      "    \"phone\": \"000-000-0000\",\n",
      "    \"street\": \"7036 Foxton Way\",\n",
      "    \"city\": \"Hanover\",\n",
      "    \"state\": \"MD\",\n",
      "    \"zip\": \"21076\",\n",
      "    \"status\": \"Active\",\n",
      "    \"medicalStatus\": \"03/05/2027\",\n",
      "    \"uniform_size\": \"s\",\n",
      "    \"county\": \"howard county\",\n",
      "    \"medical\": {\n",
      "        \"doctor_name\": \"Dr. Michelle Mcwan\",\n",
      "        \"doctor_phone\": \"443-451-1600\",\n",
      "        \"Allgeries\": NaN,\n",
      "        \"Medication\": \"Zoloft, 75mg, qd\\nClanidine, 2mg, qhs\\nExlax, 1tablet, qd\\nMiralas, 1 cap, qd\"\n",
      "    },\n",
      "    \"comments\": {\n",
      "        \"normal_comments\": \"Autism\",\n",
      "        \"award_comments\": NaN\n",
      "    }\n",
      "}\n",
      "Created nested JSON file with 1150 athletes\n",
      "\n",
      "Sample of first athlete in nested JSON:\n",
      "{\n",
      "    \"id\": 1445,\n",
      "    \"firstName\": NaN,\n",
      "    \"lastName\": NaN,\n",
      "    \"gender\": \"F\",\n",
      "    \"dateOfBirth\": \"01/30/2014\",\n",
      "    \"email\": \"na@test.com\",\n",
      "    \"phone\": \"000-000-0000\",\n",
      "    \"street\": \"7036 Foxton Way\",\n",
      "    \"city\": \"Hanover\",\n",
      "    \"state\": \"MD\",\n",
      "    \"zip\": \"21076\",\n",
      "    \"status\": \"Active\",\n",
      "    \"medicalStatus\": \"03/05/2027\",\n",
      "    \"uniform_size\": \"s\",\n",
      "    \"county\": \"howard county\",\n",
      "    \"medical\": {\n",
      "        \"doctor_name\": \"Dr. Michelle Mcwan\",\n",
      "        \"doctor_phone\": \"443-451-1600\",\n",
      "        \"Allgeries\": NaN,\n",
      "        \"Medication\": \"Zoloft, 75mg, qd\\nClanidine, 2mg, qhs\\nExlax, 1tablet, qd\\nMiralas, 1 cap, qd\"\n",
      "    },\n",
      "    \"comments\": {\n",
      "        \"normal_comments\": \"Autism\",\n",
      "        \"award_comments\": NaN\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# First, let's create the initial nested JSON file from our CSV files\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the simplified athletes data\n",
    "athletes_df = pd.read_csv('athletes_simplified.csv', encoding='latin1')\n",
    "print(f\"Loaded {len(athletes_df)} athletes from simplified CSV\")\n",
    "\n",
    "# Load the medical data\n",
    "medical_df = pd.read_csv('athlete_medical.csv', encoding='latin1')\n",
    "print(f\"Loaded {len(medical_df)} medical records\")\n",
    "\n",
    "# Load the counselors data\n",
    "counselors_df = pd.read_csv('athlete_counselors.csv', encoding='latin1')\n",
    "print(f\"Loaded {len(counselors_df)} counselor records\")\n",
    "\n",
    "# Load the comments data\n",
    "comments_df = pd.read_csv('athlete_comments.csv', encoding='latin1')\n",
    "print(f\"Loaded {len(comments_df)} comment records\")\n",
    "\n",
    "# Load the parents data\n",
    "parents_df = pd.read_csv('athlete_parents.csv', encoding='latin1')\n",
    "print(f\"Loaded {len(parents_df)} parent records\")\n",
    "\n",
    "\n",
    "# Custom JSON serializer to handle different types\n",
    "def custom_json_serializer(obj):\n",
    "    if isinstance(obj, (np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating)):\n",
    "        return str(int(obj)) if obj.is_integer() else str(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return \"\"\n",
    "    return obj\n",
    "\n",
    "# Now create a nested JSON structure\n",
    "nested_athletes = []\n",
    "\n",
    "# Process each athlete\n",
    "if not athletes_df.empty:\n",
    "    for _, athlete in athletes_df.iterrows():\n",
    "        athlete_id = athlete['id']\n",
    "        athlete_dict = athlete.to_dict()\n",
    "        \n",
    "        # Add medical information if available\n",
    "        if not medical_df.empty and 'id' in medical_df.columns:\n",
    "            medical_records = medical_df[medical_df['id'] == athlete_id]\n",
    "            if not medical_records.empty:\n",
    "                medical_info = medical_records.iloc[0].to_dict()\n",
    "                # Remove id to avoid duplication\n",
    "                if 'id' in medical_info:\n",
    "                    del medical_info['id']\n",
    "                athlete_dict['medical'] = medical_info\n",
    "        \n",
    "        # Add counselor information if available\n",
    "        if not counselors_df.empty and 'id' in counselors_df.columns:\n",
    "            counselor_records = counselors_df[counselors_df['id'] == athlete_id]\n",
    "            if not counselor_records.empty:\n",
    "                counselor_info = counselor_records.iloc[0].to_dict()\n",
    "                # Remove id to avoid duplication\n",
    "                if 'id' in counselor_info:\n",
    "                    del counselor_info['id']\n",
    "                athlete_dict['counselor'] = counselor_info\n",
    "        \n",
    "        # Add comments if available\n",
    "        if not comments_df.empty:\n",
    "            comment_records = comments_df[comments_df['AthleteId'] == athlete_id]\n",
    "            if not comment_records.empty:\n",
    "                comment_info = comment_records.iloc[0].to_dict()\n",
    "\n",
    "                # Check if comments are actually present and not just empty strings\n",
    "                has_normal_comment = pd.notna(comment_info.get('normal_comments')) and str(comment_info.get('normal_comments')).strip()\n",
    "                has_award_comment = pd.notna(comment_info.get('award_comments')) and str(comment_info.get('award_comments')).strip()\n",
    "\n",
    "                if has_normal_comment or has_award_comment:\n",
    "                    # Remove id to avoid duplication\n",
    "                    if 'AthleteId' in comment_info:\n",
    "                        del comment_info['AthleteId']\n",
    "                    athlete_dict['comments'] = comment_info\n",
    "        \n",
    "        # Add parents as an array if available\n",
    "        if not parents_df.empty:\n",
    "            parent_records = parents_df[parents_df['id'] == athlete_id]\n",
    "            if not parent_records.empty:\n",
    "                parents_list = []\n",
    "                for _, parent in parent_records.iterrows():\n",
    "                    parent_info = parent.to_dict()\n",
    "                    # Remove athlete id to avoid duplication\n",
    "                    if 'id' in parent_info:\n",
    "                        del parent_info['id']\n",
    "                    parents_list.append(parent_info)\n",
    "                athlete_dict['parents'] = parents_list\n",
    "        \n",
    "        nested_athletes.append(athlete_dict)\n",
    "\n",
    "# Write the nested data to a JSON file\n",
    "json_file_path = 'athletes_nested.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(nested_athletes, f, indent=4, default=custom_json_serializer)\n",
    "\n",
    "print(f\"Created nested JSON file with {len(nested_athletes)} athletes\")\n",
    "\n",
    "# Display a sample of the nested data if available\n",
    "if nested_athletes:\n",
    "    print(\"\\nSample of first athlete in nested JSON:\")\n",
    "    print(json.dumps(nested_athletes[0], indent=4, default=custom_json_serializer))\n",
    "else:\n",
    "    print(\"No athletes to include in nested JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dc6314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1150 athletes. Final count: 1147.\n",
      "Sample processed athlete:\n",
      "{\n",
      "    \"id\": 1034,\n",
      "    \"firstName\": \"Mona\",\n",
      "    \"lastName\": \"Abdelhalim\",\n",
      "    \"gender\": \"F\",\n",
      "    \"dateOfBirth\": \"09/25/2001\",\n",
      "    \"email\": \"na@test.com\",\n",
      "    \"phone\": \"410-282-4201\",\n",
      "    \"street\": \"8338 Goverenor Grayson Way\",\n",
      "    \"city\": \"Ellicott City\",\n",
      "    \"state\": \"MD\",\n",
      "    \"zip\": \"21043\",\n",
      "    \"status\": \"Active\",\n",
      "    \"medicalStatus\": \"08/14/2027\",\n",
      "    \"uniform_size\": \"s\",\n",
      "    \"county\": \"howard county\",\n",
      "    \"medical\": {\n",
      "        \"doctor_name\": \"Dr. Hashimi\",\n",
      "        \"doctor_phone\": \"410-997-2770\",\n",
      "        \"Allgeries\": \"\",\n",
      "        \"Medication\": \"insulin aspert, 3xday\\ndexem G7, every 10days\\nLevomyroxine, 175mg, 1xday\\nmelatonin, 1xday\\nfiber, 1xday\\nmultivitamin, 1xday\\nsuper green, black see, curcumin, cinamin\"\n",
      "    },\n",
      "    \"parents\": [\n",
      "        {\n",
      "            \"firstName\": \"Fatima\",\n",
      "            \"lastName\": \"H Kahil\",\n",
      "            \"primaryPhone\": \"410-282-4201\",\n",
      "            \"secondaryPhone\": \"410-831-7156\",\n",
      "            \"email\": \"fatmakhalil12@yahoo.com\",\n",
      "            \"street\": \"\",\n",
      "            \"city\": \"\",\n",
      "            \"state\": \"\",\n",
      "            \"zip\": \"\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "json_file_path = 'athletes_nested.json'\n",
    "\n",
    "# Step 1: Read the file content as a string\n",
    "try:\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        file_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File {json_file_path} not found.\")\n",
    "    # Create an empty list if file not found to avoid further errors, or handle appropriately\n",
    "    file_content = '[]'\n",
    "\n",
    "# Step 2: Replace standalone NaN with null to make it valid JSON\n",
    "# Using regex to replace whole word NaN only, avoiding accidental replacement in strings like 'Nancy'\n",
    "valid_json_content = re.sub(r'\\bNaN\\b', 'null', file_content)\n",
    "\n",
    "# Step 3: Parse the modified string\n",
    "try:\n",
    "    all_athletes_data = json.loads(valid_json_content)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    all_athletes_data = []  # Default to empty list on error\n",
    "\n",
    "original_athlete_count = len(all_athletes_data)\n",
    "\n",
    "# Step 4 & 5: Define a recursive function to replace None (originally NaN/null) with ''\n",
    "def replace_none_deep(item):\n",
    "    if item is None:\n",
    "        return ''\n",
    "    elif isinstance(item, dict):\n",
    "        return {k: replace_none_deep(v) for k, v in item.items()}\n",
    "    elif isinstance(item, list):\n",
    "        return [replace_none_deep(elem) for elem in item]\n",
    "    elif isinstance(item, float) and math.isnan(item):\n",
    "        return ''\n",
    "    return item\n",
    "\n",
    "processed_athletes = []\n",
    "for athlete in all_athletes_data:\n",
    "    if not isinstance(athlete, dict):  # Skip if an entry is not a dictionary\n",
    "        print(f\"Skipping non-dictionary entry: {athlete}\")\n",
    "        continue\n",
    "\n",
    "    # Check for missing name\n",
    "    first_name = athlete.get('firstName')\n",
    "    last_name = athlete.get('lastName')\n",
    "\n",
    "    # A field is considered 'missing' for filtering if it's None (originally NaN/null)\n",
    "    is_name_missing = not first_name or not last_name\n",
    "\n",
    "    if is_name_missing:\n",
    "        continue  # Skip this athlete\n",
    "    else:\n",
    "        # Replace all None values (originally NaN/null) with empty strings\n",
    "        cleaned_athlete = replace_none_deep(athlete)\n",
    "        processed_athletes.append(cleaned_athlete)\n",
    "\n",
    "final_athlete_count = len(processed_athletes)\n",
    "\n",
    "# Step 6: Write the processed data back to the JSON file\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(processed_athletes, f, indent=4)\n",
    "\n",
    "print(f\"Processed {original_athlete_count} athletes. Final count: {final_athlete_count}.\")\n",
    "if processed_athletes:\n",
    "    print(\"Sample processed athlete:\")\n",
    "    print(json.dumps(processed_athletes[0], indent=4))\n",
    "else:\n",
    "    print(\"No valid athletes found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
