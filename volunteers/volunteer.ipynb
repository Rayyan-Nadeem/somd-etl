{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28abcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f07e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VolId  Current  OneDayEvent LastContact AttendedOrientation      Last  \\\n",
      "0   1970     True        False    4/8/2017            4/7/2014         +   \n",
      "1   2400     True        False    1/2/2027           4/21/2016    Abrams   \n",
      "2   2957     True        False  10/13/2025           12/1/2022    Abrams   \n",
      "3   3075     True        False   8/29/2027           9/17/2024  Abramson   \n",
      "4   1532     True        False  10/23/2014          10/23/2011    Ackley   \n",
      "\n",
      "           First                     Street       City State  ...  \\\n",
      "0          Carla  7001 Glorious Light Place   Columbia    MD  ...   \n",
      "1  Alex Courtney       5299 Grovemont Drive   Elkridge    MD  ...   \n",
      "2         Deneen          5299 Grovemont Dr   Elkridge    MD  ...   \n",
      "3          David  2420 Fleet Street, Apt. 2  Baltimore    MD  ...   \n",
      "4           Lisa         9103 Bryant Avenue     Laurel    MD  ...   \n",
      "\n",
      "  PublicRelations CoAdmin                        Comments  \\\n",
      "0           False   False                             NaN   \n",
      "1           False   False                             NaN   \n",
      "2           False   False                             NaN   \n",
      "3           False   False                             NaN   \n",
      "4           False   False  parent of athlete Wesley Baker   \n",
      "\n",
      "                         EMail ApplicationDate  LastVolDate  LastVolEvent  \\\n",
      "0        royalty2001@yahoo.com        4/7/2014          NaN           NaN   \n",
      "1      alexabrams416@gmail.com       4/21/2016    6/15/2018           NaN   \n",
      "2    deeabrams090196@gmail.com      10/13/2022          NaN           NaN   \n",
      "3  david.s.abramson2@gmail.com       9/17/2024          NaN           NaN   \n",
      "4      lisaackley1@comcast.net       12/7/2008          NaN           NaN   \n",
      "\n",
      "   Classification                                    GeneralComments  \\\n",
      "0             NaN                                                NaN   \n",
      "1             NaN                                                NaN   \n",
      "2             NaN                                                NaN   \n",
      "3             NaN                                                NaN   \n",
      "4          Either  on-line protective behavior 11/28/08          ...   \n",
      "\n",
      "   PBC Expir Date  \n",
      "0             NaN  \n",
      "1       02-Jan-27  \n",
      "2       13-Oct-25  \n",
      "3       29-Aug-27  \n",
      "4             NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "volunteer_df = pd.read_csv('volunteers.csv', encoding='latin1')\n",
    "print(volunteer_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6f62a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def normalize_email(email):\n",
    "\n",
    "#  Normalize phone numbers: remove non-numeric characters\n",
    "def normalize_phone(phone):\n",
    "    if pd.isna(phone):\n",
    "        return 'N/A'\n",
    "    return ''.join(filter(str.isdigit, str(phone)))\n",
    "\n",
    "#if empty or not a number set to 0000000000\n",
    "def fix_phone_format(phone):\n",
    "    if pd.isna(phone) or not phone.isdigit() or len(phone) < 10:\n",
    "        return '0000000000'\n",
    "    return phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6df2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if first name or last name are empty, delete the row\n",
    "volunteer_df.dropna(subset=['First', 'Last'], how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56dc8025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         email\n",
      "0        royalty2001@yahoo.com\n",
      "1      alexabrams416@gmail.com\n",
      "2    deeabrams090196@gmail.com\n",
      "3  david.s.abramson2@gmail.com\n",
      "4      lisaackley1@comcast.net\n"
     ]
    }
   ],
   "source": [
    "# Normalize email addresses: convert to lowercase and default to 'na@test.com'\n",
    "volunteer_df.rename(columns={'EMail': 'email'}, inplace=True)\n",
    "volunteer_df['email'] = volunteer_df['email'].str.lower().fillna('na@test.com')\n",
    "print(volunteer_df[['email']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4c72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    HomePhone WorkPhone   CellPhone\n",
      "0         N/A       N/A  4109782947\n",
      "1  4107885455       N/A         N/A\n",
      "2         N/A       N/A  3012573260\n",
      "3         N/A       N/A  4104586594\n",
      "4         N/A       N/A  4434749656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "volunteer_df['HomePhone'] = volunteer_df['HomePhone'].apply(normalize_phone)\n",
    "volunteer_df['WorkPhone'] = volunteer_df['WorkPhone'].apply(normalize_phone)\n",
    "volunteer_df['CellPhone'] = volunteer_df['CellPhone'].apply(normalize_phone)\n",
    "print(volunteer_df[['HomePhone', 'WorkPhone', 'CellPhone']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bccee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        phone\n",
      "0  4109782947\n",
      "1  4107885455\n",
      "2  3012573260\n",
      "3  4104586594\n",
      "4  4434749656\n"
     ]
    }
   ],
   "source": [
    "# Prioritize phone numbers: home and cell first, work last\n",
    "def prioritize_phone(row):\n",
    "    # Return the first available phone number from the priority list\n",
    "    for phone in [row['HomePhone'], row['CellPhone'], row['WorkPhone']]:\n",
    "        if phone != 'N/A' and phone: # Check for 'N/A' and empty strings\n",
    "            return phone\n",
    "    return 'N/A' # Return 'N/A' if no valid phone is found\n",
    "\n",
    "volunteer_df['phone'] = volunteer_df.apply(prioritize_phone, axis=1)\n",
    "\n",
    "\n",
    "# go through the phone column and apply the fix_phone_format function\n",
    "if 'phone' in volunteer_df.columns:\n",
    "    volunteer_df['phone'] = volunteer_df['phone'].apply(fix_phone_format)\n",
    "\n",
    "# Remove old phone number columns\n",
    "volunteer_df.drop(columns=['HomePhone', 'WorkPhone', 'CellPhone'], inplace=True)\n",
    "\n",
    "print(volunteer_df[['phone']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b982ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Active\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n"
     ]
    }
   ],
   "source": [
    "# Rename 'Current' to 'Active' and ensure it's boolean\n",
    "volunteer_df = volunteer_df.rename(columns={'Current': 'Active'})\n",
    "volunteer_df['Active'] = volunteer_df['Active'].astype(bool)\n",
    "print(volunteer_df[['Active']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7391d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Street       City State ZipCode      sex\n",
      "0  7001 Glorious Light Place   Columbia    MD   21044  Unknown\n",
      "1       5299 Grovemont Drive   Elkridge    MD   21075  Unknown\n",
      "2          5299 Grovemont Dr   Elkridge    MD   21075  Unknown\n",
      "3  2420 Fleet Street, Apt. 2  Baltimore    MD   21224  Unknown\n",
      "4         9103 Bryant Avenue     Laurel    MD   20723  Unknown\n"
     ]
    }
   ],
   "source": [
    "# Normalize address fields and add sex\n",
    "defaults = {\n",
    "    'Street': '123 Main St',\n",
    "    'City': 'Anytown',\n",
    "    'State': 'NY',\n",
    "    'ZipCode': '20000'\n",
    "}\n",
    "for col, default in defaults.items():\n",
    "    if col in volunteer_df.columns:\n",
    "        volunteer_df[col] = volunteer_df[col].fillna('').replace(['', 'nan', 'N/A', 'None', pd.NA], default)\n",
    "\n",
    "# Add 'sex' column, default to 'Unknown' if not present\n",
    "if 'sex' not in volunteer_df.columns:\n",
    "    volunteer_df['sex'] = 'Unknown'\n",
    "else:\n",
    "    volunteer_df['sex']\n",
    "    volunteer_df['sex'] = volunteer_df['sex'].fillna('Unknown').replace('', 'Unknown')\n",
    "\n",
    "print(volunteer_df[['Street', 'City', 'State', 'ZipCode', 'sex']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7623ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dateOfBirth\n",
      "0  10/25/1964\n",
      "1  04/16/2000\n",
      "2  02/05/1965\n",
      "3  08/30/1989\n",
      "4  02/19/1959\n"
     ]
    }
   ],
   "source": [
    "# Fix 'dateOfBirth' to handle years correctly\n",
    "volunteer_df.rename(columns={'Birthdate': 'dateOfBirth'}, inplace=True)\n",
    "\n",
    "# Convert 'dateOfBirth' to datetime\n",
    "volunteer_df['dateOfBirth'] = pd.to_datetime(volunteer_df['dateOfBirth'], errors='coerce', format='%d-%b-%y')\n",
    "\n",
    "# Adjust years between 20-99 to 19XX\n",
    "def fix_year(date):\n",
    "    if pd.isna(date):\n",
    "        return None\n",
    "    if date.year > 2020:  # Adjust years above 2020 to 19XX\n",
    "        return date.replace(year=date.year - 100)\n",
    "    return date\n",
    "\n",
    "volunteer_df['dateOfBirth'] = volunteer_df['dateOfBirth'].apply(lambda x: fix_year(pd.to_datetime(x, errors='coerce')))\n",
    "\n",
    "# Format 'dateOfBirth' to MM/DD/YYYY\n",
    "volunteer_df['dateOfBirth'] = volunteer_df['dateOfBirth'].dt.strftime('%m/%d/%Y')\n",
    "print(volunteer_df[['dateOfBirth']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc0b317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PBC Expir Date\n",
      "0            NaT\n",
      "1     2027-01-02\n",
      "2     2025-10-13\n",
      "3     2027-08-29\n",
      "4            NaT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/pyf345gx7vl1vj4s17fmyrcc0000gn/T/ipykernel_68153/1550424207.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  volunteer_df['PBC Expir Date'] = pd.to_datetime(volunteer_df['PBC Expir Date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Convert 'LastContact', to datetime\n",
    "volunteer_df['PBC Expir Date'] = pd.to_datetime(volunteer_df['PBC Expir Date'], errors='coerce')\n",
    "\n",
    "print(volunteer_df[['PBC Expir Date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "#priintfullheaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0918f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns\n",
    "volunteer_df = volunteer_df.drop(columns=['ApplicationDate', 'Classification', 'LastVolDate', 'AttendedOrientation'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aaebb20",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split comments into a separate DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m comments_df = \u001b[43mvolunteer_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mComments\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGeneralComments\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.copy()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Include 'LastVolEvent' in the comments DataFrame\u001b[39;00m\n\u001b[32m      6\u001b[39m comments_df[\u001b[33m'\u001b[39m\u001b[33mLastVolEvent\u001b[39m\u001b[33m'\u001b[39m] = volunteer_df[\u001b[33m'\u001b[39m\u001b[33mLastVolEvent\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['id'] not in index\""
     ]
    }
   ],
   "source": [
    "# Split comments into a separate DataFrame\n",
    "comments_df = volunteer_df[['id', 'Comments', 'GeneralComments']].copy()\n",
    "\n",
    "\n",
    "# Include 'LastVolEvent' in the comments DataFrame\n",
    "comments_df['LastVolEvent'] = volunteer_df['LastVolEvent']\n",
    "\n",
    "# Drop 'LastVolEvent' from the main DataFrame\n",
    "final_df = volunteer_df.drop(columns=['Comments', 'GeneralComments', 'LastVolEvent'])\n",
    "comments_df.to_csv('volunteer_comments.csv', index=False)\n",
    "print(\"Comments data exported to 'volunteer_comments.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e37da",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LastVolEvent'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'LastVolEvent'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m comments_df = volunteer_df[[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mComments\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGeneralComments\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Include 'LastVolEvent' in the comments DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m comments_df[\u001b[33m'\u001b[39m\u001b[33mLastVolEvent\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mvolunteer_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLastVolEvent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Drop 'LastVolEvent' from the main DataFrame\u001b[39;00m\n\u001b[32m      9\u001b[39m final_df = volunteer_df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mComments\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mGeneralComments\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLastVolEvent\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/somd etl/etl_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'LastVolEvent'"
     ]
    }
   ],
   "source": [
    "final_df.to_csv('final_volunteer_data.csv', index=False)\n",
    "print(\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e727d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2436 volunteers from final_volunteer_data.csv\n",
      "Loaded 2436 volunteer comment records\n",
      "Loaded 2436 volunteer comment records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2436 volunteers from final_volunteer_data.csv\n",
      "Loaded 2436 volunteer comment records\n",
      "Loaded 2436 volunteer comment records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/pyf345gx7vl1vj4s17fmyrcc0000gn/T/ipykernel_67396/3946375262.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  volunteers_df[col] = pd.to_datetime(volunteers_df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2436 volunteers from final_volunteer_data.csv\n",
      "Loaded 2436 volunteer comment records\n",
      "Loaded 2436 volunteer comment records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/pyf345gx7vl1vj4s17fmyrcc0000gn/T/ipykernel_67396/3946375262.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  volunteers_df[col] = pd.to_datetime(volunteers_df[col], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created nested JSON file: volunteers_nested.json with 2436 volunteers\n",
      "\n",
      "Sample of first volunteer in nested JSON:\n",
      "{\n",
      "    \"id\": 1970,\n",
      "    \"Active\": true,\n",
      "    \"OneDayEvent\": false,\n",
      "    \"LastContact\": \"04/08/2017\",\n",
      "    \"AttendedOrientation\": \"04/07/2014\",\n",
      "    \"Last\": \"+\",\n",
      "    \"First\": \"Carla\",\n",
      "    \"Street\": \"7001 Glorious Light Place\",\n",
      "    \"City\": \"Columbia\",\n",
      "    \"State\": \"MD\",\n",
      "    \"ZipCode\": \"21044\",\n",
      "    \"dateOfBirth\": \"10/25/1964\",\n",
      "    \"email\": \"royalty2001@yahoo.com\",\n",
      "    \"ApplicationDate\": \"04/07/2014\",\n",
      "    \"LastVolDate\": \"\",\n",
      "    \"Classification\": NaN,\n",
      "    \"PBC Expir Date\": \"\",\n",
      "    \"phone\": \"4109782947\",\n",
      "    \"sex\": \"Unknown\",\n",
      "    \"comments\": {\n",
      "        \"Comments\": \"\",\n",
      "        \"GeneralComments\": \"\",\n",
      "        \"LastVolEvent\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to fix phone numbers (convert floats to strings without decimals)\n",
    "def fix_phone_format(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Custom JSON serializer to handle different types\n",
    "def custom_json_serializer(obj):\n",
    "    if pd.isna(obj) or obj is pd.NaT:  # Check for NaN/NaT first\n",
    "        return \"\"\n",
    "    elif isinstance(obj, (np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating)):\n",
    "        # NaN is already handled by the pd.isna() check above.\n",
    "        # This branch is for actual float numbers.\n",
    "        return str(int(obj)) if obj.is_integer() else str(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.Timestamp)):\n",
    "        return obj.strftime('%m/%d/%Y') # Format dates as MM/DD/YYYY\n",
    "    return obj\n",
    "\n",
    "# Load the processed volunteer data\n",
    "try:\n",
    "    volunteers_df = pd.read_csv('final_volunteer_data.csv', encoding='latin1')\n",
    "    print(f\"Loaded {len(volunteers_df)} volunteers from final_volunteer_data.csv\")\n",
    "\n",
    "    # Convert relevant date columns to datetime objects for proper serialization\n",
    "    date_columns_to_parse = ['ApplicationDate', 'LastVolDate', 'LastContact', 'AttendedOrientation', 'PBC Expir Date']\n",
    "    for col in date_columns_to_parse:\n",
    "        if col in volunteers_df.columns:\n",
    "            volunteers_df[col] = pd.to_datetime(volunteers_df[col], errors='coerce')\n",
    "            # The custom_json_serializer will handle formatting Timestamps to MM-DD-YYYY\n",
    "            # and NaT (from errors='coerce') to \"\"\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: final_volunteer_data.csv not found\")\n",
    "    volunteers_df = pd.DataFrame()\n",
    "\n",
    "# Fix phone numbers in volunteers dataframe\n",
    "if not volunteers_df.empty:\n",
    "    if 'phone' in volunteers_df.columns:\n",
    "        volunteers_df['phone'] = volunteers_df['phone'].apply(fix_phone_format)\n",
    "\n",
    "# Load the volunteer comments data\n",
    "try:\n",
    "    volunteer_comments_df = pd.read_csv('volunteer_comments.csv', encoding='latin1')\n",
    "    print(f\"Loaded {len(volunteer_comments_df)} volunteer comment records\")\n",
    "    # Replace all NaN values in the comments DataFrame with empty strings\n",
    "    volunteer_comments_df = volunteer_comments_df.fillna(\"\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: volunteer_comments.csv not found\")\n",
    "    volunteer_comments_df = pd.DataFrame().fillna(\"\") # Ensure empty df also has no NaNs if used later\n",
    "\n",
    "# Create a nested JSON structure for volunteers\n",
    "nested_volunteers = []\n",
    "\n",
    "if not volunteers_df.empty:\n",
    "    for _, volunteer in volunteers_df.iterrows():\n",
    "        volunteer_id = volunteer['id']\n",
    "        volunteer_dict = volunteer.to_dict()\n",
    "        \n",
    "        # Add comments if available\n",
    "        if not volunteer_comments_df.empty and 'id' in volunteer_comments_df.columns:\n",
    "            comment_records = volunteer_comments_df[volunteer_comments_df['id'] == volunteer_id]\n",
    "            if not comment_records.empty:\n",
    "                # Convert all columns except 'id' to a dictionary\n",
    "                comment_info = comment_records.drop(columns=['id']).iloc[0].to_dict()\n",
    "                volunteer_dict['comments'] = comment_info\n",
    "        \n",
    "        nested_volunteers.append(volunteer_dict)\n",
    "\n",
    "# Write the nested data to a JSON file\n",
    "json_file_path = 'volunteers_nested.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(nested_volunteers, f, indent=4, default=custom_json_serializer)\n",
    "\n",
    "print(f\"Created nested JSON file: {json_file_path} with {len(nested_volunteers)} volunteers\")\n",
    "\n",
    "# Display a sample of the nested data if available\n",
    "if nested_volunteers:\n",
    "    print(\"\\nSample of first volunteer in nested JSON:\")\n",
    "    print(json.dumps(nested_volunteers[0], indent=4, default=custom_json_serializer))\n",
    "else:\n",
    "    print(\"No volunteers to include in nested JSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
